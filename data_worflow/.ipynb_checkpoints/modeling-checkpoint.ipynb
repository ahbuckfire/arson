{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "\n",
    "sys.path.append('/Users/abuckfire/side-projects/arson')\n",
    "import static_data.fire_codes.lookup_tables as codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARSON_PATH = os.path.join(\"..\", \"static_data\", \"nfirs_arson\")\n",
    "POPULATION_PATH = os.path.join(\"..\", \"static_data\", \"zipcodes\", \"nst-est2017-popchg2010_2017.csv\")\n",
    "\n",
    "COUNT = \"count\"\n",
    "POP_STATE = \"NAME\"\n",
    "STATE = \"state\"\n",
    "POPULATION_FIELD = \"POPESTIMATE\"\n",
    "POPULATION = \"population\"\n",
    "YEARS_AVAILABLE = map(str, range(2009, 2015))\n",
    "FILE_PREFIX = \"nfirs_arson_\"\n",
    "DENSITY = \"pop_density\"\n",
    "YEAR = \"year\"\n",
    "FIELDS = [STATE, COUNT, POPULATION, YEAR, DENSITY]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_arson_density(population_df, year, state_rename_udf):\n",
    "    arson_df = spark.read.csv(os.path.join(ARSON_PATH, FILE_PREFIX + year + \".csv\"), header=True)\\\n",
    "                .select([STATE, \"fdid\"])\\\n",
    "                .groupBy(STATE).count()\n",
    "\n",
    "    y1 = year if year != \"2009\" else \"2010\" #adjusting for lack of 2009 population data\n",
    "    population_for_year = population_df.select(POP_STATE, POPULATION_FIELD + y1)\\\n",
    "                                       .withColumnRenamed(POP_STATE, STATE)\\\n",
    "                                       .withColumnRenamed(POPULATION_FIELD + y1, POPULATION)\n",
    "\n",
    "    joined_data = arson_df.withColumn(STATE, state_rename_udf(arson_df[STATE]))\\\n",
    "                          .join(population_for_year, STATE)\\\n",
    "                          .withColumn(\"year\", functions.lit(year))\n",
    "    \n",
    "    return joined_data.withColumn(DENSITY, joined_data[COUNT] * 100000 / joined_data[POPULATION])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_density_to_elastic():\n",
    "    population_df = spark.read.csv(POPULATION_PATH, header=True)\n",
    "    state_rename_udf = functions.udf(lambda x: codes.STATES[x.strip()] if codes.STATES.get(x) else 0, \"string\")\n",
    "    \n",
    "    densities = calculate_arson_density(population_df, \"2009\", state_rename_udf)\n",
    "    print densities.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
